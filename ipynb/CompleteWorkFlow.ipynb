{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An example of dynamically growing and shrinking an Analytic partition to handle larger workloads.\n",
    "\n",
    "This example assume that Moab/Tourge (PBS) is running and we have a ccm queue to handle allocations. In the fur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/12/10 14:25:04 INFO client.RMProxy: Connecting to ResourceManager at nid00379/10.128.1.126:8032\n",
      "Total Nodes:4\n",
      "         Node-Id\t     Node-State\tNode-Http-Address\tNumber-of-Running-Containers\n",
      "  nid00382:35733\t        RUNNING\t    nid00382:8042\t                           2\n",
      "  nid00380:40375\t        RUNNING\t    nid00380:8042\t                           0\n",
      "  nid00383:38910\t        RUNNING\t    nid00383:8042\t                           1\n",
      "  nid00381:52626\t        RUNNING\t    nid00381:8042\t                           0\n"
     ]
    }
   ],
   "source": [
    "!yarn node -list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90.9 M   .flink\r\n",
      "178.3 M  .sparkStaging\r\n",
      "610.4 M  HiBench\r\n",
      "335.5 K  artofwar.txt\r\n",
      "558.8 G  bigdata\r\n",
      "191.6 K  hamlet.txt\r\n",
      "18.4 M   ints.parquet\r\n",
      "68.1 M   pagecounts\r\n",
      "3.0 K    passwd\r\n",
      "19.1 M   text8_linessmall\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -du -h "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!qstat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a simple PBS/Moab script which dynamically increases the \"pool\" size, then after a predetermined time will bing the pool back to the default size (4) or the user can send a signal to the batch jobs to kill the session and revert it back to the default 4 nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26052.sdb\n",
      "Job ID                    Name             User            Time Use S Queue\n",
      "------------------------- ---------------- --------------- -------- - -----\n",
      "26052.sdb                  jsparks-test     jsparks                0 R ccm_queue      \n"
     ]
    }
   ],
   "source": [
    "!(cd /home/users/jsparks/tmp ; rm -f jsparks-test.o*; qsub -q ccm_queue -l nodes=12:ppn=1 job.sh; sleep 2; qstat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/12/10 13:14:36 INFO client.RMProxy: Connecting to ResourceManager at nid00379/10.128.1.126:8032\n",
      "Total Nodes:16\n",
      "         Node-Id\t     Node-State\tNode-Http-Address\tNumber-of-Running-Containers\n",
      "  nid00016:24361\t        RUNNING\t    nid00016:8042\t                           0\n",
      "  nid00007:14786\t        RUNNING\t    nid00007:8042\t                           0\n",
      "  nid00005:10089\t        RUNNING\t    nid00005:8042\t                           0\n",
      "  nid00014:23136\t        RUNNING\t    nid00014:8042\t                           0\n",
      "  nid00382:35733\t        RUNNING\t    nid00382:8042\t                           0\n",
      "  nid00006:11929\t        RUNNING\t    nid00006:8042\t                           0\n",
      "  nid00380:40375\t        RUNNING\t    nid00380:8042\t                           2\n",
      "  nid00383:38910\t        RUNNING\t    nid00383:8042\t                           0\n",
      "  nid00015:25506\t        RUNNING\t    nid00015:8042\t                           0\n",
      "  nid00008:13215\t        RUNNING\t    nid00008:8042\t                           0\n",
      "  nid00011:19756\t        RUNNING\t    nid00011:8042\t                           0\n",
      "  nid00013:21545\t        RUNNING\t    nid00013:8042\t                           0\n",
      "  nid00381:52626\t        RUNNING\t    nid00381:8042\t                           4\n",
      "  nid00009:15939\t        RUNNING\t    nid00009:8042\t                           0\n",
      "  nid00012:18547\t        RUNNING\t    nid00012:8042\t                           0\n",
      "  nid00010:17595\t        RUNNING\t    nid00010:8042\t                           0\n"
     ]
    }
   ],
   "source": [
    "!sleep 10; qstat -u ${USER}; yarn node -list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sneak peak at what your job is doing you can tail/cat you job file in /var/spool/torque/spool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/tmp/*.parquet': No such file or directory\n",
      "rm: `/tmp/*.seq': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -rm -R -skipTrash /tmp/*.parquet\n",
    "!hadoop fs -rm -R -skipTrash /tmp/*.seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have more nodes, you can run larger hadoop/spark jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** This is python3 so some functions have been renamed/depricated etc ... Just an FYI..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "sc.defaultParallelism\n",
    "\n",
    "# Set the number of executor cores to match the system's allocation\n",
    "data = sc.parallelize(range(1000)).flatMap(lambda x: [Row(a=random.randint(1, 10), num=random.randint(1, 100), str=(\"a\" * random.randint(1, 30))) for i in range(10000)])\n",
    "dataTable = sqlContext.createDataFrame(data)\n",
    "# Depricated call\n",
    "#dataTable.saveAsParquetFile(\"/tmp/ints1.parquet\")\n",
    "dataTable.write.parquet(\"/tmp/ints1.parquet\")\n",
    "\n",
    "data = sc.parallelize(range(1000)).flatMap(lambda x: [Row(a=random.randint(1, 10), num=random.randint(1, 100)) for i in range(10000)])\n",
    "dataTable = sqlContext.createDataFrame(data)\n",
    "# Depricated call\n",
    "#dataTable.saveAsParquetFile(\"/tmp/ints2.parquet\")\n",
    "dataTable.write.parquet(\"/tmp/ints2.parquet\")\n",
    "\n",
    "data = sc.parallelize(range(1000)).flatMap(lambda x: [(random.randint(1, 10), random.randint(1, 100)) for i in range(10000)])\n",
    "data.saveAsNewAPIHadoopFile('hdfs://nid00379:8020/tmp/ints3.seq',\n",
    "                         'org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat', # Output format class\n",
    "                         conf = {\n",
    "                              'mapreduce.output.fileoutputformat.compress'       : 'true',\n",
    "                              'mapreduce.output.fileoutputformat.compress.codec' : 'org.apache.hadoop.io.compress.DefaultCodec',\n",
    "                              'mapreduce.output.fileoutputformat.compress.type'  : 'BLOCK'\n",
    "                              }\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 items\n",
      "drwx------   - jsparks supergroup          0 2015-10-27 15:06 /tmp/hadoop-yarn\n",
      "drwx-wx-wx   - jsparks supergroup          0 2015-11-20 13:15 /tmp/hive\n",
      "drwxr-xr-x   - jsparks supergroup          0 2015-12-10 10:11 /tmp/ints1.parquet\n",
      "drwxr-xr-x   - jsparks supergroup          0 2015-12-10 10:11 /tmp/ints2.parquet\n",
      "drwxr-xr-x   - jsparks supergroup          0 2015-12-10 10:11 /tmp/ints3.seq\n",
      "drwxrwxrwx   - jsparks supergroup          0 2015-11-18 11:37 /tmp/twitter\n",
      "0      /tmp/ints1.parquet/_SUCCESS\n",
      "340    /tmp/ints1.parquet/_common_metadata\n",
      "1.1 K  /tmp/ints1.parquet/_metadata\n",
      "9.2 M  /tmp/ints1.parquet/part-r-00000-cd276803-b726-401d-901c-a1583bf15c21.gz.parquet\n",
      "9.2 M  /tmp/ints1.parquet/part-r-00001-cd276803-b726-401d-901c-a1583bf15c21.gz.parquet\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -ls /tmp\n",
    "!hadoop fs -du -h  /tmp/ints1.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/12/10 13:35:51 INFO client.RMProxy: Connecting to ResourceManager at nid00379/10.128.1.126:8032\n",
      "Total Nodes:4\n",
      "         Node-Id\t     Node-State\tNode-Http-Address\tNumber-of-Running-Containers\n",
      "  nid00382:35733\t        RUNNING\t    nid00382:8042\t                           0\n",
      "  nid00380:40375\t        RUNNING\t    nid00380:8042\t                           2\n",
      "  nid00383:38910\t        RUNNING\t    nid00383:8042\t                           0\n",
      "  nid00381:52626\t        RUNNING\t    nid00381:8042\t                           4\n"
     ]
    }
   ],
   "source": [
    "!yarn node -list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the tests\n",
    "See http://0x0fff.com/spark-dataframes-are-faster-arent-they/\n",
    "for a better writeup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test #1 Runtime:    5.30 sec\n",
      "Test #2 Runtime:   24.64 sec\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    " \n",
    "def mappart(part):\n",
    "    counters = dict()\n",
    "    for x,y in part:\n",
    "        if not x in counters:\n",
    "            counters[x] = [0, 0]\n",
    "        counters[x][0] += 1\n",
    "        counters[x][1] += y\n",
    "    return counters.iteritems()\n",
    " \n",
    "def fn():\n",
    "    # Test 1\n",
    "    runtime = 0.0\n",
    "    for i in range(5):\n",
    "        start = time.time()\n",
    "        data = sqlContext.read.load(\"/tmp/ints1.parquet\")\n",
    "        #data = sqlContext.load(\"/tmp/ints1.parquet\")\n",
    "        data.groupBy(\"a\").agg(col(\"a\"), avg(\"num\")).collect()\n",
    "        runtime += time.time() - start\n",
    "    print (\"Test #1 Runtime: %7.2f sec\" % (runtime / 5.0))\n",
    "    # Test 2\n",
    "    runtime = 0.0\n",
    "    for i in range(5):\n",
    "        start = time.time()\n",
    "        pdata = sqlContext.read.load(\"/tmp/ints2.parquet\")\n",
    "        #pdata = sqlContext.load(\"/tmp/ints2.parquet\")\n",
    "        sum_count = pdata.map(lambda x: (x.a, [x.num, 1])).reduceByKey(lambda x, y:[x[0] + y[0], x[1] + y[1]]).collect()\n",
    "        sum_count = [(x[0], float(x[1][0]) / x[1][1]) for x in sum_count]\n",
    "        runtime += time.time() - start\n",
    "    print (\"Test #2 Runtime: %7.2f sec\" % (runtime / 5.0))\n",
    " \n",
    "fn()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark (Spark 1.5.1-py27)",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
