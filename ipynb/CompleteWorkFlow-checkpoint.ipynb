{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/12/01 10:00:15 INFO client.RMProxy: Connecting to ResourceManager at nid00379/10.128.1.126:8032\n",
      "Total Nodes:4\n",
      "         Node-Id\t     Node-State\tNode-Http-Address\tNumber-of-Running-Containers\n",
      "  nid00382:34211\t        RUNNING\t    nid00382:8042\t                           3\n",
      "  nid00383:39260\t        RUNNING\t    nid00383:8042\t                           2\n",
      "  nid00380:37678\t        RUNNING\t    nid00380:8042\t                           1\n",
      "  nid00381:45955\t        RUNNING\t    nid00381:8042\t                           0\n"
     ]
    }
   ],
   "source": [
    "!yarn node -list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90.9 M   .flink\r\n",
      "356.7 M  .sparkStaging\r\n",
      "335.5 K  artofwar.txt\r\n",
      "558.8 G  bigdata\r\n",
      "191.6 K  hamlet.txt\r\n",
      "18.4 M   ints.parquet\r\n",
      "68.1 M   pagecounts\r\n",
      "3.0 K    passwd\r\n",
      "19.1 M   text8_linessmall\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -du -h "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!qstat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a simple PBS/Moab script which dynamically increases the \"pool\" size, then after a predetermined time will bing the pool back to the default size (4) or the user can send a signal to the batch jobs to kill the session and revert it back to the default 4 nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25974.sdb\n",
      "Job ID                    Name             User            Time Use S Queue\n",
      "------------------------- ---------------- --------------- -------- - -----\n",
      "25974.sdb                  jsparks-test     jsparks                0 R ccm_queue      \n"
     ]
    }
   ],
   "source": [
    "!(cd /home/users/jsparks/tmp ; rm -f jsparks-test.o*; qsub -q ccm_queue -l nodes=12:ppn=1 job.sh ; sleep 2; qstat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sneak peak at what your job is doing you can tail/cat you job file in /var/spool/torque/spool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /tmp/ints1.parquet\n",
      "Deleted /tmp/ints2.parquet\n",
      "Deleted /tmp/ints3.seq\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -rm -R -skipTrash /tmp/*.parquet\n",
    "!hadoop fs -rm -R -skipTrash /tmp/*.seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have more nodes, you can run larger hadoop/spark jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** This is python3 so some functions have been renamed/depricated etc ... Just an FYI..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    " \n",
    "data = sc.parallelize(range(1000)).flatMap(lambda x: [Row(a=random.randint(1, 10), num=random.randint(1, 100), str=(\"a\" * random.randint(1, 30))) for i in range(10000)])\n",
    "dataTable = sqlContext.createDataFrame(data)\n",
    "# Depricated call\n",
    "#dataTable.saveAsParquetFile(\"/tmp/ints1.parquet\")\n",
    "dataTable.write.parquet(\"/tmp/ints1.parquet\")\n",
    "\n",
    "data = sc.parallelize(range(1000)).flatMap(lambda x: [Row(a=random.randint(1, 10), num=random.randint(1, 100)) for i in range(10000)])\n",
    "dataTable = sqlContext.createDataFrame(data)\n",
    "# Depricated call\n",
    "#dataTable.saveAsParquetFile(\"/tmp/ints2.parquet\")\n",
    "dataTable.write.parquet(\"/tmp/ints2.parquet\")\n",
    "\n",
    "data = sc.parallelize(range(1000)).flatMap(lambda x: [(random.randint(1, 10), random.randint(1, 100)) for i in range(10000)])\n",
    "data.saveAsNewAPIHadoopFile('hdfs://nid00379:8020/tmp/ints3.seq',\n",
    "                         'org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat', # Output format class\n",
    "                         conf = {\n",
    "                              'mapreduce.output.fileoutputformat.compress'       : 'true',\n",
    "                              'mapreduce.output.fileoutputformat.compress.codec' : 'org.apache.hadoop.io.compress.DefaultCodec',\n",
    "                              'mapreduce.output.fileoutputformat.compress.type'  : 'BLOCK'\n",
    "                              }\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 items\n",
      "drwx------   - jsparks supergroup          0 2015-10-27 15:06 /tmp/hadoop-yarn\n",
      "drwx-wx-wx   - jsparks supergroup          0 2015-11-20 13:15 /tmp/hive\n",
      "drwxr-xr-x   - jsparks supergroup          0 2015-12-01 10:10 /tmp/ints1.parquet\n",
      "drwxr-xr-x   - jsparks supergroup          0 2015-12-01 10:11 /tmp/ints2.parquet\n",
      "drwxr-xr-x   - jsparks supergroup          0 2015-12-01 10:11 /tmp/ints3.seq\n",
      "drwxrwxrwx   - jsparks supergroup          0 2015-11-18 11:37 /tmp/twitter\n",
      "0      /tmp/ints1.parquet/_SUCCESS\n",
      "340    /tmp/ints1.parquet/_common_metadata\n",
      "1.1 K  /tmp/ints1.parquet/_metadata\n",
      "9.2 M  /tmp/ints1.parquet/part-r-00000-b1ed89b6-b08f-4688-9120-11054a7c2550.gz.parquet\n",
      "9.2 M  /tmp/ints1.parquet/part-r-00001-b1ed89b6-b08f-4688-9120-11054a7c2550.gz.parquet\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -ls /tmp\n",
    "!hadoop fs -du -h  /tmp/ints1.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/12/01 10:19:56 INFO client.RMProxy: Connecting to ResourceManager at nid00379/10.128.1.126:8032\n",
      "Total Nodes:4\n",
      "         Node-Id\t     Node-State\tNode-Http-Address\tNumber-of-Running-Containers\n",
      "  nid00382:34211\t        RUNNING\t    nid00382:8042\t                           3\n",
      "  nid00383:39260\t        RUNNING\t    nid00383:8042\t                           2\n",
      "  nid00380:37678\t        RUNNING\t    nid00380:8042\t                           1\n",
      "  nid00381:45955\t        RUNNING\t    nid00381:8042\t                           0\n"
     ]
    }
   ],
   "source": [
    "!yarn node -list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the tests\n",
    "See http://0x0fff.com/spark-dataframes-are-faster-arent-they/\n",
    "for a better writeup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test #1 Runtime:    5.30 sec\n",
      "Test #2 Runtime:   24.64 sec\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    " \n",
    "def mappart(part):\n",
    "    counters = dict()\n",
    "    for x,y in part:\n",
    "        if not x in counters:\n",
    "            counters[x] = [0, 0]\n",
    "        counters[x][0] += 1\n",
    "        counters[x][1] += y\n",
    "    return counters.iteritems()\n",
    " \n",
    "def fn():\n",
    "    # Test 1\n",
    "    runtime = 0.0\n",
    "    for i in range(5):\n",
    "        start = time.time()\n",
    "        data = sqlContext.read.load(\"/tmp/ints1.parquet\")\n",
    "        #data = sqlContext.load(\"/tmp/ints1.parquet\")\n",
    "        data.groupBy(\"a\").agg(col(\"a\"), avg(\"num\")).collect()\n",
    "        runtime += time.time() - start\n",
    "    print (\"Test #1 Runtime: %7.2f sec\" % (runtime / 5.0))\n",
    "    # Test 2\n",
    "    runtime = 0.0\n",
    "    for i in range(5):\n",
    "        start = time.time()\n",
    "        pdata = sqlContext.read.load(\"/tmp/ints2.parquet\")\n",
    "        #pdata = sqlContext.load(\"/tmp/ints2.parquet\")\n",
    "        sum_count = pdata.map(lambda x: (x.a, [x.num, 1])).reduceByKey(lambda x, y:[x[0] + y[0], x[1] + y[1]]).collect()\n",
    "        sum_count = [(x[0], float(x[1][0]) / x[1][1]) for x in sum_count]\n",
    "        runtime += time.time() - start\n",
    "    print (\"Test #2 Runtime: %7.2f sec\" % (runtime / 5.0))\n",
    " \n",
    "fn()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark (Spark 1.5.1-py35)",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
